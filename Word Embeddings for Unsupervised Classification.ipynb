{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb549792",
   "metadata": {},
   "source": [
    "# Unsupervised Text Classification with Word Embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bca78252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 15:52:38.272104: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-19 15:52:38.272544: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8f95be57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-02 16:43:39.498196: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-02 16:43:39.498930: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 185, in _run_module_as_main\n",
      "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 144, in _get_module_details\n",
      "    return _get_module_details(pkg_main_name, error)\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 111, in _get_module_details\n",
      "    __import__(pkg_name)\n",
      "  File \"/home/echao/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/spacy/__init__.py\", line 11, in <module>\n",
      "    from thinc.api import prefer_gpu, require_gpu, require_cpu  # noqa: F401\n",
      "  File \"/home/echao/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/thinc/__init__.py\", line 6, in <module>\n",
      "    from .util import require_cpu\n",
      "  File \"/home/echao/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/thinc/util.py\", line 38, in <module>\n",
      "    import tensorflow.experimental.dlpack\n",
      "  File \"/home/echao/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/tensorflow/__init__.py\", line 41, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"/home/echao/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python.feature_column import feature_column_lib as feature_column\n",
      "  File \"/home/echao/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/tensorflow/python/feature_column/feature_column_lib.py\", line 22, in <module>\n",
      "    from tensorflow.python.feature_column.feature_column import *\n",
      "  File \"/home/echao/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/tensorflow/python/feature_column/feature_column.py\", line 147, in <module>\n",
      "    from tensorflow.python.layers import base\n",
      "  File \"/home/echao/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/tensorflow/python/layers/base.py\", line 20, in <module>\n",
      "    from tensorflow.python.keras.legacy_tf_layers import base\n",
      "  File \"/home/echao/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/tensorflow/python/keras/__init__.py\", line 22, in <module>\n",
      "    from tensorflow.python.keras import distribute\n",
      "  File \"/home/echao/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/tensorflow/python/keras/distribute/__init__.py\", line 18, in <module>\n",
      "    from tensorflow.python.keras.distribute import sidecar_evaluator\n",
      "  File \"/home/echao/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/tensorflow/python/keras/distribute/sidecar_evaluator.py\", line 24, in <module>\n",
      "    from tensorflow.python.training.tracking import util as tracking_util\n",
      "  File \"/home/echao/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py\", line 110, in <module>\n",
      "    def register_session_provider(session_provider):\n",
      "  File \"/home/echao/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/tensorflow/python/util/tf_export.py\", line 344, in __call__\n",
      "    _NAME_TO_SYMBOL_MAPPING[name] = func\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg\n",
    "# pip uninstall en-core-web-lg\n",
    "# To remove the model after c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e221c354",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22757/957677298.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_lg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     )\n",
      "\u001b[0;32m~/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "386ef24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(text):\n",
    "    '''Utility function for removing punctuation, lowercasing and removing extra whitespace'''\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bae6becf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(tokens, nlp):\n",
    "    \"\"\"Returns the centroid of the embeddings for the given tokens of a document\n",
    "    \n",
    "    Out-of-vocabulary tokens and stopwords are cast aside.\n",
    "    Zero vector is returned if no tokens are valid.\n",
    "    \"\"\"\n",
    "    lexemes = (nlp.vocab[token] for token in tokens)\n",
    "    \n",
    "    vectors = np.asarray([\n",
    "        lexeme.vector\n",
    "        for lexeme in lexemes\n",
    "        if lexeme.has_vector\n",
    "        and not lexeme.is_stop\n",
    "        and len(lexeme.text) > 1\n",
    "    ])\n",
    "\n",
    "    if len(vectors) > 0:\n",
    "        centroid = vectors.mean(axis=0)\n",
    "    else:\n",
    "        width = nlp.meta['vectors']['width']  # typically 300\n",
    "        centroid = np.zeros(width)\n",
    "\n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d699293e",
   "metadata": {},
   "source": [
    "**Note:**  A _lexeme_ is a basic abstract unit of meaning, a unit of morphological analysis in linguistics that roughly corresponds to a set of forms taken by a single root word. In spacy, we can access a word's lexeme with nlp.vocab(token), which may have its own, unique word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af237f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example0 = \"I have never been more angry at myself for forgetting my own girlfriend's birthday.\"\n",
    "example1 = \"Dogs are such beautiful creatures, I find they area  great companion.\"\n",
    "exampl2 = \"No all espresso bars are good. Last week I tried one of the worst cups of coffee and it cost me $6.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd181d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEXEMES: <generator object embed.<locals>.<genexpr> at 0x7f8190b45e40>\n"
     ]
    }
   ],
   "source": [
    "tokens0 = example0.split(' ')\n",
    "centroid0 = embed(tokens0, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ce2dd7",
   "metadata": {},
   "source": [
    "### Classifying as 'good' or 'bad'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26d0ca1",
   "metadata": {},
   "source": [
    "- All that remains is to find the closest neighbor to the centroid.\n",
    "- For this task we could use the NearestNeighbors class from `sklearn`.\n",
    "- We must have the embeddings corresponding to each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a3c8197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_embeddings(labels, nlp):\n",
    "    '''Given a list of label names returns the corresponding word embeddings.\n",
    "        Handles label names with more than two words.\n",
    "    '''\n",
    "    label_embeddings = np.asarray([\n",
    "        embed(label.split(' '), nlp)\n",
    "        for label in labels\n",
    "    ])\n",
    "\n",
    "    return label_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9be3de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEXEMES: <generator object embed.<locals>.<genexpr> at 0x7f81cb9e5f20>\n",
      "LEXEMES: <generator object embed.<locals>.<genexpr> at 0x7f81cb9e5f20>\n"
     ]
    }
   ],
   "source": [
    "label_names = ['good', 'bad']\n",
    "\n",
    "label_embeddings = get_label_embeddings(label_names, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "827ab662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(n_neighbors=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nb = NearestNeighbors(n_neighbors=1)\n",
    "nb.fit(label_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccc1248a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bad'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_label = nb.kneighbors([centroid0], return_distance=False)[0, 0]\n",
    "label_names[closest_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85bd6bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Dogs are such beautiful creatures, I find they area  great companion.\n",
      "LEXEMES: <generator object embed.<locals>.<genexpr> at 0x7f8190b45f20>\n",
      "Result: good\n"
     ]
    }
   ],
   "source": [
    "tokens1 = example1.split(' ')\n",
    "print(\"Document\", example1)\n",
    "centroid1 = embed(tokens1, nlp)\n",
    "closest_label = nb.kneighbors([centroid1], return_distance=False)[0, 0]\n",
    "print(\"Result:\", label_names[closest_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234816c3",
   "metadata": {},
   "source": [
    "We can see that our first document `example0` got classified correctly!\n",
    "This is amazing because we did not even have to train any supervised model ourselves.\n",
    "\n",
    "The million dolar question is, _how well does this perform for N documents?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af9b257",
   "metadata": {},
   "source": [
    "## Evaluating Performance with a big dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f4cbda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/amazon_baby.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "85c103f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1771ad49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Planetwise Wipe Pouch</td>\n",
       "      <td>it came early and was not disappointed. i love...</td>\n",
       "      <td>5</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annas Dream Full Quilt with 2 Shams</td>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "      <td>5</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>This is a product well worth the purchase.  I ...</td>\n",
       "      <td>5</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>5</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>When the Binky Fairy came to our house, we did...</td>\n",
       "      <td>5</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A Tale of Baby's Days with Peter Rabbit</td>\n",
       "      <td>Lovely book, it's bound tightly so you may not...</td>\n",
       "      <td>4</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Baby Tracker&amp;reg; - Daily Childcare Journal, S...</td>\n",
       "      <td>Perfect for new parents. We were able to keep ...</td>\n",
       "      <td>5</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Baby Tracker&amp;reg; - Daily Childcare Journal, S...</td>\n",
       "      <td>A friend of mine pinned this product on Pinter...</td>\n",
       "      <td>5</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Baby Tracker&amp;reg; - Daily Childcare Journal, S...</td>\n",
       "      <td>This has been an easy way for my nanny to reco...</td>\n",
       "      <td>4</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Baby Tracker&amp;reg; - Daily Childcare Journal, S...</td>\n",
       "      <td>I love this journal and our nanny uses it ever...</td>\n",
       "      <td>4</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name  \\\n",
       "1                               Planetwise Wipe Pouch   \n",
       "2                 Annas Dream Full Quilt with 2 Shams   \n",
       "3   Stop Pacifier Sucking without tears with Thumb...   \n",
       "4   Stop Pacifier Sucking without tears with Thumb...   \n",
       "5   Stop Pacifier Sucking without tears with Thumb...   \n",
       "6             A Tale of Baby's Days with Peter Rabbit   \n",
       "7   Baby Tracker&reg; - Daily Childcare Journal, S...   \n",
       "8   Baby Tracker&reg; - Daily Childcare Journal, S...   \n",
       "9   Baby Tracker&reg; - Daily Childcare Journal, S...   \n",
       "10  Baby Tracker&reg; - Daily Childcare Journal, S...   \n",
       "\n",
       "                                               review  rating sentiment  \n",
       "1   it came early and was not disappointed. i love...       5      good  \n",
       "2   Very soft and comfortable and warmer than it l...       5      good  \n",
       "3   This is a product well worth the purchase.  I ...       5      good  \n",
       "4   All of my kids have cried non-stop when I trie...       5      good  \n",
       "5   When the Binky Fairy came to our house, we did...       5      good  \n",
       "6   Lovely book, it's bound tightly so you may not...       4      good  \n",
       "7   Perfect for new parents. We were able to keep ...       5      good  \n",
       "8   A friend of mine pinned this product on Pinter...       5      good  \n",
       "9   This has been an easy way for my nanny to reco...       4      good  \n",
       "10  I love this journal and our nanny uses it ever...       4      good  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'] = np.where(df['rating'] > 3, 'good', 'bad')\n",
    "\n",
    "eval_df = df[df['rating'] != 3]\n",
    "neutral_df = df[df['rating'] == 3]\n",
    "\n",
    "eval_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed503de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(doc, nlp, nb, label_names):\n",
    "    doc = clean_string(doc)\n",
    "    tokens = doc.split()[:50]  # only the first 50 tokens\n",
    "    centroid = embed(tokens, nlp)\n",
    "    closest_label = nb.kneighbors([centroid1], return_distance=False)[0, 0]\n",
    "    return label_names[closest_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0591f10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_290/2117002597.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_df['predicted_label'] = eval_df['review'].apply(lambda x: predict(str(x), nlp, nb, label_names))\n"
     ]
    }
   ],
   "source": [
    "eval_df['predicted_label'] = eval_df['review'].apply(lambda x: predict(x, nlp, nb, label_names))\n",
    "\n",
    "# For 2,225 documents, it takes about 2 seconds\n",
    "# For a 50,000 length dataset, it would take abt 44 secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2437d3",
   "metadata": {},
   "source": [
    "For demostrating the predictive power of this technique we will use the heuristic of rating to provide true labels to compare against.\n",
    "\n",
    "- We will see the accuracy of predicting a 4+ star review as good and a 2- star review as bad.\n",
    "- For 3 star reviews, we will see which one is more prevalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ba1f2b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Planetwise Wipe Pouch</td>\n",
       "      <td>it came early and was not disappointed. i love...</td>\n",
       "      <td>5</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annas Dream Full Quilt with 2 Shams</td>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "      <td>5</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>This is a product well worth the purchase.  I ...</td>\n",
       "      <td>5</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>5</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>When the Binky Fairy came to our house, we did...</td>\n",
       "      <td>5</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "1                              Planetwise Wipe Pouch   \n",
       "2                Annas Dream Full Quilt with 2 Shams   \n",
       "3  Stop Pacifier Sucking without tears with Thumb...   \n",
       "4  Stop Pacifier Sucking without tears with Thumb...   \n",
       "5  Stop Pacifier Sucking without tears with Thumb...   \n",
       "\n",
       "                                              review  rating sentiment  \\\n",
       "1  it came early and was not disappointed. i love...       5      good   \n",
       "2  Very soft and comfortable and warmer than it l...       5      good   \n",
       "3  This is a product well worth the purchase.  I ...       5      good   \n",
       "4  All of my kids have cried non-stop when I trie...       5      good   \n",
       "5  When the Binky Fairy came to our house, we did...       5      good   \n",
       "\n",
       "  predicted_label  \n",
       "1            good  \n",
       "2            good  \n",
       "3            good  \n",
       "4            good  \n",
       "5            good  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3a2e07d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    count = 0\n",
    "    for p, l in zip(predictions, labels):\n",
    "        count += p == l\n",
    "    \n",
    "    return count*1.0 / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a1b13064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of embeddings classifier 0.8411233448474381\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(eval_df['predicted_label'], eval_df['sentiment'])\n",
    "print(\"Accuracy of embeddings classifier\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f82f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Echarse un clavado en el 15%\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44635b82",
   "metadata": {},
   "source": [
    "### ...and the Neutral reviews?\n",
    "\n",
    "Just for curiosity we want to see the majority class among neutral reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "278e5238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_class(predictions):\n",
    "    good = 0\n",
    "    bad = 0\n",
    "    for p in predictions:\n",
    "        if p=='good':\n",
    "            good += 1\n",
    "        else:\n",
    "            bad += 1\n",
    "           \n",
    "    print(f\"Good: {good} ({good/len(predictions)}) \\nBad: {bad} ({bad/len(predictions)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7f840a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/echao/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/pandas/util/_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n",
      "/tmp/ipykernel_290/180695842.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  neutral_df['review'] = neutral_df['review'].astype('string')\n",
      "/tmp/ipykernel_290/180695842.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  neutral_df['predicted_label'] = neutral_df['review'].apply(lambda x: predict(x, nlp, nb, label_names))\n"
     ]
    }
   ],
   "source": [
    "neutral_df.dropna(inplace=True)\n",
    "neutral_df['review'] = neutral_df['review'].astype('string')\n",
    "neutral_df['predicted_label'] = neutral_df['review'].apply(lambda x: predict(x, nlp, nb, label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9b6ba62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16705\n",
      "Good: 16705 (1.0) \n",
      "Bad: 0 (0.0)\n"
     ]
    }
   ],
   "source": [
    "print(len(neutral_df))\n",
    "majority_class(neutral_df['predicted_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d496fdba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
