{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dda55d78",
   "metadata": {},
   "source": [
    "# Word Embeddings para clasificación binaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a7d76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 14:38:52.665492: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-19 14:38:52.665573: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://maxhalford.github.io/blog/unsupervised-text-classification/'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "# Inspiración:\n",
    "'''https://maxhalford.github.io/blog/unsupervised-text-classification/'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a70acc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/echao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95753d0",
   "metadata": {},
   "source": [
    "#### Cargar la Data: reseñas de Amazon en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "387e9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use test data just because of size\n",
    "df = pd.read_json('../data/dataset_es_test.json', lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "997e4c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review_title'] + ' ' + df['review_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77f043e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['stars'] != 3]  # quitar reseñas neutrales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f5e197",
   "metadata": {},
   "source": [
    "#### Cargar Word Embeddings (Ahora usando \"Spanish Billion Word Corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33cd9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "we = KeyedVectors.load_word2vec_format('../data/sbw_vectors.bin', limit=200000, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7a028a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'humano'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pruebas de we\n",
    "\n",
    "we.doesnt_match(['perro', 'gato', 'conejo', 'humano'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "975390e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4132543"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "we.similarity(\"asco\", \"malo\")   # muy disimilares?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab519baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50715035\n",
      "0.4186036\n"
     ]
    }
   ],
   "source": [
    "print(we.similarity(\"maravilla\", \"feliz\"))\n",
    "print(we.similarity(\"maravilla\", \"malo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8d9ecee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3905842\n"
     ]
    }
   ],
   "source": [
    "print(we.similarity(\"Bueno\", \"bien\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6fe89fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17277518\n"
     ]
    }
   ],
   "source": [
    "print(we.similarity(\"como\", \"alimento\"))  # parece que estos WE no capturan bien el significado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e79bf8",
   "metadata": {},
   "source": [
    "#### Functions to clean and tokenize documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cabb6615",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stopwords = set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1753c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    Removes punctuation, \n",
    "    lowercases, --> word vectors work better in lowercase\n",
    "    removes extra whitespace\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation+'¿¡'))\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35cc38a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hola cómo estás'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"¡Hola! ¿¿Cómo estás?!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02032757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenize(text):\n",
    "    '''\n",
    "    Tokenizes the text, \n",
    "    removes stopwords and \n",
    "    words with less than 2 chars\n",
    "    '''\n",
    "    text = clean_text(text)\n",
    "    tokens = text.split()\n",
    "    return [t for t in tokens if t not in Stopwords and len(t)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4558c612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hola', 'nunca', 'pensé', 'podrías', 'indicar', 'cómo']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tokenize(\"¡¡hola! nunca pensé   en eso, ¿me podrías indicar cómo?  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bf1e5c",
   "metadata": {},
   "source": [
    "#### Function to compute a document's word embedding centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0cfe6299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedWE(tokens, we):\n",
    "    '''\n",
    "    Misma validación de clean_tokenize: no stopwords, tiene que estar en el vocabulario\n",
    "    '''\n",
    "    vectors = np.asarray([\n",
    "        we[token]\n",
    "        for token in tokens\n",
    "        if token in we\n",
    "        and len(token)>1\n",
    "        and token not in Stopwords\n",
    "    ])\n",
    "\n",
    "    if len(vectors) > 0:\n",
    "        centroid = vectors.mean(axis=0) # mean vector across columns; 300 \n",
    "    else:\n",
    "        centroid = np.zeros(300)         # width is 300\n",
    "        \n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0866aa7b",
   "metadata": {},
   "source": [
    "#### Obtain Labels and their Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6baeb462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_centroids(labels, we):\n",
    "    '''\n",
    "    Regresa los embeddings \n",
    "    correspondientes a cada uno de los nombres de clase.\n",
    "    El nombre de la clase puede tener varias palabras.\n",
    "    '''\n",
    "    label_centroids = np.asarray([\n",
    "        embedWE(name.split(), we)\n",
    "        for name in labels\n",
    "    ])\n",
    "    \n",
    "    return label_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ebaa01",
   "metadata": {},
   "source": [
    "### Selección de Clusters (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9d4eb3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_title</th>\n",
       "      <th>language</th>\n",
       "      <th>product_category</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>prediction</th>\n",
       "      <th>spacy_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>es_0038754</td>\n",
       "      <td>product_es_0113523</td>\n",
       "      <td>reviewer_es_0580071</td>\n",
       "      <td>1</td>\n",
       "      <td>no me llego el articulo me lo mando por correo...</td>\n",
       "      <td>no me llego</td>\n",
       "      <td>es</td>\n",
       "      <td>wireless</td>\n",
       "      <td>no me llego no me llego el articulo me lo mand...</td>\n",
       "      <td>decepcionante</td>\n",
       "      <td>feliz bueno maravilla</td>\n",
       "      <td>decepcionado enojado malo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>es_0748979</td>\n",
       "      <td>product_es_0017036</td>\n",
       "      <td>reviewer_es_0819733</td>\n",
       "      <td>1</td>\n",
       "      <td>la mensajería horrible, no compro mas</td>\n",
       "      <td>amazon sigue sin cumplir en las entregas</td>\n",
       "      <td>es</td>\n",
       "      <td>home</td>\n",
       "      <td>amazon sigue sin cumplir en las entregas la me...</td>\n",
       "      <td>decepcionante</td>\n",
       "      <td>feliz bueno maravilla</td>\n",
       "      <td>decepcionado enojado malo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    review_id          product_id          reviewer_id  stars  \\\n",
       "0  es_0038754  product_es_0113523  reviewer_es_0580071      1   \n",
       "1  es_0748979  product_es_0017036  reviewer_es_0819733      1   \n",
       "\n",
       "                                         review_body  \\\n",
       "0  no me llego el articulo me lo mando por correo...   \n",
       "1              la mensajería horrible, no compro mas   \n",
       "\n",
       "                               review_title language product_category  \\\n",
       "0                               no me llego       es         wireless   \n",
       "1  amazon sigue sin cumplir en las entregas       es             home   \n",
       "\n",
       "                                              review      sentiment  \\\n",
       "0  no me llego no me llego el articulo me lo mand...  decepcionante   \n",
       "1  amazon sigue sin cumplir en las entregas la me...  decepcionante   \n",
       "\n",
       "              prediction                 spacy_pred  \n",
       "0  feliz bueno maravilla  decepcionado enojado malo  \n",
       "1  feliz bueno maravilla  decepcionado enojado malo  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label_names = ['feliz', 'decepcionado']   # muy influyente en el resultado\n",
    "#label_names = ['feliz bueno maravilla', 'decepcionado enojado malo']   # muy influyente en el resultado\n",
    "#label_names = ['bien', 'mal']   # muy influyente en el resultado\n",
    "label_names = ['excelente', 'decepcionante']   # muy influyente en el resultado\n",
    "\n",
    "\n",
    "# Etiquetar para fines de medición\n",
    "df['sentiment'] = np.where(df['stars'] > 3, label_names[0], label_names[1])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b0b007b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(n_neighbors=1)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# With WE\n",
    "labels_we = get_label_centroids(label_names, we)\n",
    "# With Spacy\n",
    "labels_spacy = get_spacy_labels(label_names, nlp)\n",
    "\n",
    "nb = NearestNeighbors(n_neighbors=1)\n",
    "nb.fit(labels_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ab6c5001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFY\n",
    "def predict(doc, we, nb, label_names):\n",
    "    tokens = clean_tokenize(doc)  # cap at 50 tokens, could influence result\n",
    "    centroid = embedWE(tokens, we)\n",
    "    closest_label = nb.kneighbors([centroid], return_distance=False)[0][0]\n",
    "    return label_names[closest_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "83d337db",
   "metadata": {},
   "outputs": [],
   "source": [
    "example0 = df['review'][0]\n",
    "example1 = df['review'][1]\n",
    "example2 = df['review'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "25586a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'feliz bueno maravilla'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(example0, we, nb, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "87300d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'feliz bueno maravilla'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(example1, we, nb, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "df6d745c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'feliz bueno maravilla'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(example2, we, nb, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f25a27",
   "metadata": {},
   "source": [
    "#### Probar predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2697f28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prediction'] = df['review'].apply(lambda x: predict(x, we, nb, label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "51cef72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(predictions, label_col):\n",
    "    predictions = np.asarray(predictions)\n",
    "    label_col = np.asarray(label_col)\n",
    "    num_correct = 0\n",
    "    for i in range(len(predictions)):\n",
    "        num_correct += 1*(predictions[i]==label_col[i])\n",
    "\n",
    "    return num_correct/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2221f9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50275"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(df['prediction'], df['sentiment'])\n",
    "\n",
    "# mala precisión!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa09f333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed71dba5",
   "metadata": {},
   "source": [
    "### SpaCy es model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c3dc43",
   "metadata": {},
   "source": [
    "Descargamos el modelo pre-entrenado de Spacy para español, `es_core_news_lg`.\n",
    "Este modelo contiene embeddings tanto para palabras como para lexemas en español, a través de la clase Vocab.\n",
    "Siguiendo el ejemplo en inglés, usamos los vectores de los lexemas.\n",
    "No obstante, los tokens devueltos por el objeto `nlp` tienen una propiedad `vector`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "71ec3c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "de93704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(tokens, nlp):\n",
    "    \"\"\"\n",
    "    Returns the centroid of embeddings of the given tokens\n",
    "    Out-of-vocabulary and stopwords are ignored\n",
    "    If no tokens are valid, zero vector is returned\n",
    "    \"\"\"\n",
    "    lexemes = (nlp.vocab[token] for token in tokens)\n",
    "    \n",
    "    vectors = np.asarray([\n",
    "        lexeme.vector\n",
    "        for lexeme in lexemes\n",
    "        if lexeme.has_vector\n",
    "        and not lexeme.is_stop\n",
    "        and len(lexeme.text) > 1\n",
    "    ])\n",
    "\n",
    "    if len(vectors) > 0:\n",
    "        centroid = vectors.mean(axis=0)\n",
    "    else:\n",
    "        width = nlp.meta['vectors']['width']  # fastText Wikipedia Spanish\n",
    "        centroid = np.zeros(width)\n",
    "\n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ad9f4547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedText(text, nlp):\n",
    "    tokens = nlp(text)\n",
    "    vectors = np.asarray([\n",
    "        tok.vector\n",
    "        for tok in tokens\n",
    "        if tok.has_vector\n",
    "        and not tok.is_stop\n",
    "        and len(tok.text) > 1\n",
    "    ])\n",
    "    if len(vectors) > 0:\n",
    "        centroid = vectors.mean(axis=0)\n",
    "    else:\n",
    "        width = nlp.meta['vectors']['width']  # fastText Wikipedia Spanish\n",
    "        centroid = np.zeros(width)\n",
    "\n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d9d0c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "example0 = \"Jamás había estado tan enojado por haberme olvidado del cumpleaños de mi novia.\"\n",
    "example1 = \"Los perros son criaturas hermosas, yo pienso que hacen una muy buena compañía.\"\n",
    "example2 = \"No todos los bares de café son buenos. La semana pasada fui a Quentin y estuvo terrible!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c8dd3da1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['excelente', 'decepcionante']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5cababba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spacy_labels(label_names, nlp):\n",
    "    '''\n",
    "    Dada una lista de nombres de clase, regresa los embeddings \n",
    "    correspondientes a cada una.\n",
    "    El nombre de la clase puede tener varias palabras.\n",
    "    '''\n",
    "    \n",
    "    label_embeddings = np.asarray([\n",
    "        embed(name.split(), nlp)\n",
    "        for name in label_names\n",
    "    ])\n",
    "    \n",
    "    return label_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2a1d7258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSpaCy(doc, nlp, nb, label_names):\n",
    "    tokens = clean_tokenize(doc)  # cap at 50 tokens, could influence result\n",
    "    centroid = embed(tokens, nlp)\n",
    "    closest_label = nb.kneighbors([centroid], return_distance=False)[0][0]\n",
    "    return label_names[closest_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "03df110d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(n_neighbors=1)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_s = get_spacy_labels(label_names, nlp)\n",
    "\n",
    "nb = NearestNeighbors(n_neighbors=1)\n",
    "nb.fit(labels_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ba282e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decepcionante'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictSpaCy(example0, nlp, nb, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fbe80a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decepcionante'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictSpaCy(example1, nlp, nb, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c0a837b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decepcionante'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictSpaCy(example0, nlp, nb, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4bc92712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51625"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Spacy Predictions\n",
    "df['spacy_pred'] = df['review'].apply(lambda x: predictSpaCy(x, nlp, nb, label_names))\n",
    "\n",
    "\n",
    "get_accuracy(df['spacy_pred'], df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412bec6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807b85e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce72b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48e0d65d",
   "metadata": {},
   "source": [
    "# Clasificación no supervisada con spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8c95333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"excelente\", \"decepcionante\"]\n",
    "label_embeddings = get_label_embeddings(label_names, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ea92916d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(n_neighbors=2)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nb = NearestNeighbors(n_neighbors=2)\n",
    "nb.fit(label_embeddings)  # label centroids are training data for clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "de72e75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decepcionante'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Jamás había estado tan enojado por haberme olvidado del cumpleaños de mi novia\"\n",
    "closest_label = nb.kneighbors([centroid0], return_distance=False)[0, 0]\n",
    "label_names[closest_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d90e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be330d50",
   "metadata": {},
   "source": [
    "### Otro intento: ahora con 500-Dimensional vectors, 5-window [Bilbao, Almeida]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01330ce9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.models.deprecated'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1417/381212591.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mFILE_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/mnt/c/Users/ericj/Downloads/keyed_vectors/complete.kv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwe3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_lifecycle_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1458\u001b[0m     \"\"\"\n\u001b[1;32m   1459\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# needed because loading from S3 doesn't support readline()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim.models.deprecated'"
     ]
    }
   ],
   "source": [
    "import gensim.models\n",
    "FILE_PATH = '/mnt/c/Users/ericj/Downloads/keyed_vectors/complete.kv'\n",
    "\n",
    "we3 = KeyedVectors.load(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd2d19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
