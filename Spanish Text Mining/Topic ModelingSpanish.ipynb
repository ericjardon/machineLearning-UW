{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94fc8484",
   "metadata": {},
   "source": [
    "# Pequeña práctica con un dataset de noticias en español\n",
    "\n",
    "El dataset proviene de Kaggle\n",
    "Se trata de un conjunto de 1000 noticias falsas y 1000 noticias verdaderas.\n",
    "\n",
    "\n",
    "[Dataset Noticias](https://www.kaggle.com/arseniitretiakov/noticias-falsas-en-espaol?select=fakes1000.csv)\n",
    "\n",
    "[Basic tasks with spacy](https://necronet.github.io/Spacy-getting-started-in-spanish/)\n",
    "\n",
    "### Usar amazon reviews y replicar la últimas práctica de Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6115478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from gensim.corpora import Dictionary\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "994d9dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>Algunas de las voces extremistas más conocida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>Después de casi dos años y medio de luchas po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>Dos periodistas birmanos de la agencia Reuter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>El Cuerpo Nacional de Policía ha detenido a c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>El desfile de la firma en Roma se convierte e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>True</td>\n",
       "      <td>El Consejo de Gobierno ha dado su visto bueno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>True</td>\n",
       "      <td>Investigadores valencianos han desarrollado u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>True</td>\n",
       "      <td>Los arrestados actuaban en coches y en establ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>True</td>\n",
       "      <td>El Rey ha encargado este miércoles a Pedro Sá...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>True</td>\n",
       "      <td>Las pruebas realizadas en el Centro Nacional ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      class                                               Text\n",
       "0      True   Algunas de las voces extremistas más conocida...\n",
       "1      True   Después de casi dos años y medio de luchas po...\n",
       "2      True   Dos periodistas birmanos de la agencia Reuter...\n",
       "3      True   El Cuerpo Nacional de Policía ha detenido a c...\n",
       "4      True   El desfile de la firma en Roma se convierte e...\n",
       "...     ...                                                ...\n",
       "1995   True   El Consejo de Gobierno ha dado su visto bueno...\n",
       "1996   True   Investigadores valencianos han desarrollado u...\n",
       "1997   True   Los arrestados actuaban en coches y en establ...\n",
       "1998   True   El Rey ha encargado este miércoles a Pedro Sá...\n",
       "1999   True   Las pruebas realizadas en el Centro Nacional ...\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/fakes1000.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e04e07b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Algunas de las voces extremistas más conocidas de EE.UU., cuentas asociadas con al movimiento de la conocida como alt-right o ligadas a grupos antisemitas o xenófobos , han sido eliminadas de un plumazo de la redes sociales Facebook e Instagram. Se trat'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f7a4f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tokenization, tagging, lemmatization, stemming and NER\n",
    "import spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a802a697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Algunas de las voces extremistas más conocidas de EE.UU., cuentas asociadas con al movimiento de la conocida como alt-right o ligadas a grupos antisemitas o xenófobos , han sido eliminadas de un plumazo de la redes sociales Facebook e Instagram. Se trat\n"
     ]
    }
   ],
   "source": [
    "sample = df['Text'][0]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51e648c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "alguno\n",
      "de\n",
      "el\n",
      "voz\n",
      "extremista\n",
      "más\n",
      "conocido\n",
      "de\n",
      "EE.UU.\n",
      ",\n",
      "cuenta\n",
      "asociado\n",
      "con\n",
      "al\n",
      "movimiento\n",
      "de\n",
      "el\n",
      "conocido\n",
      "como\n",
      "alt-right\n",
      "o\n",
      "ligado\n",
      "a\n",
      "grupo\n",
      "antisemita\n",
      "o\n",
      "xenófobo\n",
      ",\n",
      "haber\n",
      "ser\n",
      "eliminar\n",
      "de\n",
      "uno\n",
      "plumazo\n",
      "de\n",
      "el\n",
      "red\n",
      "social\n",
      "Facebook\n",
      "e\n",
      "Instagram\n",
      ".\n",
      "él\n",
      "trat\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(sample)\n",
    "for w in doc:\n",
    "    print(w.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a04cfb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words(\"spanish\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db012e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_tokenize_lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    # Avoid empty word in beggining with doc[1:]\n",
    "    return [word.lemma_ for word in doc[1:] if word.lemma_ not in stopwords]\n",
    "    \n",
    "    \n",
    "df['tokens'] = df['Text'].apply(normalize_tokenize_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80b9abcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [alguno, voz, extremista, conocido, EE.UU., ,,...\n",
       "1       [después, casi, dos, año, medio, lucha, políti...\n",
       "2       [dos, periodista, birmano, agencia, Reuters, s...\n",
       "3       [Cuerpo, Nacional, Policía, haber, detener, cu...\n",
       "4       [desfile, firma, Roma, convertir, oda, liberta...\n",
       "                              ...                        \n",
       "1995    [Consejo, Gobierno, haber, dar, visto, bueno, ...\n",
       "1996    [Investigadores, valenciano, haber, desarrolla...\n",
       "1997    [arrestado, actuar, coche, establecimiento, Po...\n",
       "1998    [Rey, haber, encargar, miércoles, Pedro, Sánch...\n",
       "1999    [prueba, realizado, Centro, Nacional, Microbio...\n",
       "Name: tokens, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a64ceb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(df['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "917697dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.compactify()   # reassign integer ids to every word\n",
    "# filter out words that appear in less than 2 documents, thos appearing in more than 97% of corpus, and keep all other words\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.97, keep_n=None)\n",
    "dictionary.compactify()   # reassign integer ids to every word, erasing gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a29890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(tokens) for tokens in df['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557d727b",
   "metadata": {},
   "source": [
    "### Once we have the corpus\n",
    "1. HDP: We could infer the number of topics in the corpus with an HDP model, but is not always easy to interpret\n",
    "2. LSI: this model tells us the optimal number of topics by looking at a coherence measurement.\n",
    "3. Once we have a good estimate for number of topics we can do LDA model\n",
    "https://medium.com/@armandj.olivares/topic-modeling-on-spanish-text-f7a5e998fb90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc237cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
