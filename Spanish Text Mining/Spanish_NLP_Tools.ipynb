{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c11ef469",
   "metadata": {},
   "source": [
    "# Recursos para Minería de Texto en Español\n",
    "En este notebook exploramos algunas de las herramientas de uso libre disponibles para realizar tareas básicas de NLP con texto en español.\n",
    "\n",
    "Eric Andrés Jardón Chao <br/>\n",
    "14 de octubre de 2021 <br/>\n",
    "\n",
    "[Repositorio de Recursos NLP en español](https://github.com/dav009/awesome-spanish-nlp) <br/>\n",
    "\n",
    "[NLP básico con Spacy en español](https://necronet.github.io/Spacy-getting-started-in-spanish/)\n",
    "\n",
    "[POS Tagging & Lemmatization - CLARIN](https://www.clarin.eu/resource-families/tools-part-speech-tagging-and-lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52f6b2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 15:22:34.782759: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-15 15:22:34.783372: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abf3a06",
   "metadata": {},
   "source": [
    "# NLTK + Open Multilingual WordNet\n",
    "[Open Multilingual Wordnet](http://compling.hss.ntu.edu.sg/omw/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f48b4013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0f1959b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/echao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw to /home/echao/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2ba4560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['margen', 'orilla', 'vera']\n",
      "['computadora', 'ordenador', 'procesador']\n"
     ]
    }
   ],
   "source": [
    "print(wn.synsets('bank')[0].lemma_names('spa')\n",
    ")\n",
    "print(wn.synsets('computer')[0].lemma_names('spa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c3fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2be2323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'la',\n",
       " 'que',\n",
       " 'el',\n",
       " 'en',\n",
       " 'y',\n",
       " 'a',\n",
       " 'los',\n",
       " 'del',\n",
       " 'se',\n",
       " 'las',\n",
       " 'por',\n",
       " 'un',\n",
       " 'para',\n",
       " 'con',\n",
       " 'no',\n",
       " 'una',\n",
       " 'su',\n",
       " 'al',\n",
       " 'lo',\n",
       " 'como',\n",
       " 'más',\n",
       " 'pero',\n",
       " 'sus',\n",
       " 'le',\n",
       " 'ya',\n",
       " 'o',\n",
       " 'este',\n",
       " 'sí',\n",
       " 'porque',\n",
       " 'esta',\n",
       " 'entre',\n",
       " 'cuando',\n",
       " 'muy',\n",
       " 'sin',\n",
       " 'sobre',\n",
       " 'también',\n",
       " 'me',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'donde',\n",
       " 'quien',\n",
       " 'desde',\n",
       " 'todo',\n",
       " 'nos',\n",
       " 'durante',\n",
       " 'todos',\n",
       " 'uno',\n",
       " 'les',\n",
       " 'ni',\n",
       " 'contra',\n",
       " 'otros',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'ante',\n",
       " 'ellos',\n",
       " 'e',\n",
       " 'esto',\n",
       " 'mí',\n",
       " 'antes',\n",
       " 'algunos',\n",
       " 'qué',\n",
       " 'unos',\n",
       " 'yo',\n",
       " 'otro',\n",
       " 'otras',\n",
       " 'otra',\n",
       " 'él',\n",
       " 'tanto',\n",
       " 'esa',\n",
       " 'estos',\n",
       " 'mucho',\n",
       " 'quienes',\n",
       " 'nada',\n",
       " 'muchos',\n",
       " 'cual',\n",
       " 'poco',\n",
       " 'ella',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'algunas',\n",
       " 'algo',\n",
       " 'nosotros',\n",
       " 'mi',\n",
       " 'mis',\n",
       " 'tú',\n",
       " 'te',\n",
       " 'ti',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'ellas',\n",
       " 'nosotras',\n",
       " 'vosotros',\n",
       " 'vosotras',\n",
       " 'os',\n",
       " 'mío',\n",
       " 'mía',\n",
       " 'míos',\n",
       " 'mías',\n",
       " 'tuyo',\n",
       " 'tuya',\n",
       " 'tuyos',\n",
       " 'tuyas',\n",
       " 'suyo',\n",
       " 'suya',\n",
       " 'suyos',\n",
       " 'suyas',\n",
       " 'nuestro',\n",
       " 'nuestra',\n",
       " 'nuestros',\n",
       " 'nuestras',\n",
       " 'vuestro',\n",
       " 'vuestra',\n",
       " 'vuestros',\n",
       " 'vuestras',\n",
       " 'esos',\n",
       " 'esas',\n",
       " 'estoy',\n",
       " 'estás',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estáis',\n",
       " 'están',\n",
       " 'esté',\n",
       " 'estés',\n",
       " 'estemos',\n",
       " 'estéis',\n",
       " 'estén',\n",
       " 'estaré',\n",
       " 'estarás',\n",
       " 'estará',\n",
       " 'estaremos',\n",
       " 'estaréis',\n",
       " 'estarán',\n",
       " 'estaría',\n",
       " 'estarías',\n",
       " 'estaríamos',\n",
       " 'estaríais',\n",
       " 'estarían',\n",
       " 'estaba',\n",
       " 'estabas',\n",
       " 'estábamos',\n",
       " 'estabais',\n",
       " 'estaban',\n",
       " 'estuve',\n",
       " 'estuviste',\n",
       " 'estuvo',\n",
       " 'estuvimos',\n",
       " 'estuvisteis',\n",
       " 'estuvieron',\n",
       " 'estuviera',\n",
       " 'estuvieras',\n",
       " 'estuviéramos',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuviese',\n",
       " 'estuvieses',\n",
       " 'estuviésemos',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estando',\n",
       " 'estado',\n",
       " 'estada',\n",
       " 'estados',\n",
       " 'estadas',\n",
       " 'estad',\n",
       " 'he',\n",
       " 'has',\n",
       " 'ha',\n",
       " 'hemos',\n",
       " 'habéis',\n",
       " 'han',\n",
       " 'haya',\n",
       " 'hayas',\n",
       " 'hayamos',\n",
       " 'hayáis',\n",
       " 'hayan',\n",
       " 'habré',\n",
       " 'habrás',\n",
       " 'habrá',\n",
       " 'habremos',\n",
       " 'habréis',\n",
       " 'habrán',\n",
       " 'habría',\n",
       " 'habrías',\n",
       " 'habríamos',\n",
       " 'habríais',\n",
       " 'habrían',\n",
       " 'había',\n",
       " 'habías',\n",
       " 'habíamos',\n",
       " 'habíais',\n",
       " 'habían',\n",
       " 'hube',\n",
       " 'hubiste',\n",
       " 'hubo',\n",
       " 'hubimos',\n",
       " 'hubisteis',\n",
       " 'hubieron',\n",
       " 'hubiera',\n",
       " 'hubieras',\n",
       " 'hubiéramos',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubiese',\n",
       " 'hubieses',\n",
       " 'hubiésemos',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'habiendo',\n",
       " 'habido',\n",
       " 'habida',\n",
       " 'habidos',\n",
       " 'habidas',\n",
       " 'soy',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'somos',\n",
       " 'sois',\n",
       " 'son',\n",
       " 'sea',\n",
       " 'seas',\n",
       " 'seamos',\n",
       " 'seáis',\n",
       " 'sean',\n",
       " 'seré',\n",
       " 'serás',\n",
       " 'será',\n",
       " 'seremos',\n",
       " 'seréis',\n",
       " 'serán',\n",
       " 'sería',\n",
       " 'serías',\n",
       " 'seríamos',\n",
       " 'seríais',\n",
       " 'serían',\n",
       " 'era',\n",
       " 'eras',\n",
       " 'éramos',\n",
       " 'erais',\n",
       " 'eran',\n",
       " 'fui',\n",
       " 'fuiste',\n",
       " 'fue',\n",
       " 'fuimos',\n",
       " 'fuisteis',\n",
       " 'fueron',\n",
       " 'fuera',\n",
       " 'fueras',\n",
       " 'fuéramos',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fuese',\n",
       " 'fueses',\n",
       " 'fuésemos',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'sintiendo',\n",
       " 'sentido',\n",
       " 'sentida',\n",
       " 'sentidos',\n",
       " 'sentidas',\n",
       " 'siente',\n",
       " 'sentid',\n",
       " 'tengo',\n",
       " 'tienes',\n",
       " 'tiene',\n",
       " 'tenemos',\n",
       " 'tenéis',\n",
       " 'tienen',\n",
       " 'tenga',\n",
       " 'tengas',\n",
       " 'tengamos',\n",
       " 'tengáis',\n",
       " 'tengan',\n",
       " 'tendré',\n",
       " 'tendrás',\n",
       " 'tendrá',\n",
       " 'tendremos',\n",
       " 'tendréis',\n",
       " 'tendrán',\n",
       " 'tendría',\n",
       " 'tendrías',\n",
       " 'tendríamos',\n",
       " 'tendríais',\n",
       " 'tendrían',\n",
       " 'tenía',\n",
       " 'tenías',\n",
       " 'teníamos',\n",
       " 'teníais',\n",
       " 'tenían',\n",
       " 'tuve',\n",
       " 'tuviste',\n",
       " 'tuvo',\n",
       " 'tuvimos',\n",
       " 'tuvisteis',\n",
       " 'tuvieron',\n",
       " 'tuviera',\n",
       " 'tuvieras',\n",
       " 'tuviéramos',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuviese',\n",
       " 'tuvieses',\n",
       " 'tuviésemos',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'teniendo',\n",
       " 'tenido',\n",
       " 'tenida',\n",
       " 'tenidos',\n",
       " 'tenidas',\n",
       " 'tened']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hapaxes()  palabras que ocurren una sola vez en el texto\n",
    "nltk.corpus.stopwords.words(\"spanish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6c212b",
   "metadata": {},
   "source": [
    "## Recursos de la UNAM\n",
    "Del Curso de Procesamiento de Lenguaje Natural con Python de la UNAM.\n",
    "Podemos encontrar Corpus y otros tutoriales para trabajar con texto en español.\n",
    "- [Sitio de Corpus Lingüísticos de la UNAM](http://www.iling.unam.mx/corpus/)\n",
    "- [Curso de Procesamiento de Lenguaje Natural con Python de la UNAM](http://www.corpus.unam.mx/cursopln/)\n",
    "\n",
    "Un **corpus** es un conjunto de documentos destinado a la investigación; sea texto, video, audio u otro formato.\n",
    "Para el curso de NLP de la UNAM se utilizó un corpus de documentos legales en español. (quizás no es el mejor caso pues es una forma muy particular de hablar, distinta del español conversacional o coloquial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9703755",
   "metadata": {},
   "source": [
    "# Part-Of-Speech Taggers en Español\n",
    "\n",
    "Un **POST Tagger** es un programa que lee texto en algún lenguaje y asigna partes de discurso correspondientes a cada palabra (y cada token) tales como: _sustantivo_, _verbo_, _adjetivo_, etcétera.\n",
    "\n",
    "- [Stanford NLP Group](https://nlp.stanford.edu/software/tagger.shtml)\n",
    "    - _Download full Stanford Tagger_ (~129MB)\n",
    "    -  Descomprimir el zip, ubicar `stanford_postagger.jar`\n",
    "    - Ubicar la carpeta /models y adentro el archivo `spanish.tagger`\n",
    "    - Se requiere la ruta completa de ambos archivos para utilizarlos con Python y NLTK\n",
    "    - [**Guía**](https://nlp.stanford.edu/software/spanish-faq.shtml#tagset)\n",
    "\n",
    "\"Python: 2020s advice: You should always use a Python interface to the CoreNLPServer for performant use in Python. For NLTK, use the nltk.parse.corenlp module. Historically, NLTK (2.0+) contains an interface to the Stanford POS tagger. The original version was written by Nitin Madnani: documentation (note: in old versions, manually set the character encoding or you get ASCII!), code, on Github. After a while there was a better CoreNLPPOSTagger class.\"\n",
    "\n",
    "- [Tree POSTagger - University of Stuttgart](https://www.cis.lmu.de/~schmid/tools/TreeTagger/)\n",
    "- [SpaCy](https://spacy.io/models), que implementa casi todo de lo que hemos visto\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6aebd66b",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "Could not find stanford-postagger.jar jar file at /mnt/c/Users/ericj/Downloads/.../stanford-postagger.jar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5532/1646864731.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mjarfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/mnt/c/Users/ericj/Downloads/.../stanford-postagger.jar\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStanfordPOSTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjarfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_filename, path_to_jar, encoding, verbose, java_options)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;34m\"StanfordPOSTagger or StanfordNERTagger?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             )\n\u001b[0;32m---> 70\u001b[0;31m         self._stanford_jar = find_jar(\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_JAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_stanford_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         )\n",
      "\u001b[0;32m~/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/nltk/internals.py\u001b[0m in \u001b[0;36mfind_jar\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[0mis_regex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m ):\n\u001b[0;32m--> 845\u001b[0;31m     return next(\n\u001b[0m\u001b[1;32m    846\u001b[0m         find_jar_iter(\n\u001b[1;32m    847\u001b[0m             \u001b[0mname_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_regex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/nltk/internals.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m             raise LookupError(\n\u001b[0m\u001b[1;32m    732\u001b[0m                 \u001b[0;34mf\"Could not find {name_pattern} jar file at {path_to_jar}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             )\n",
      "\u001b[0;31mLookupError\u001b[0m: Could not find stanford-postagger.jar jar file at /mnt/c/Users/ericj/Downloads/.../stanford-postagger.jar"
     ]
    }
   ],
   "source": [
    "## DO NOT RUN -- files not found\n",
    "\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "\n",
    "tokens = \"El chavo del ocho es un programa famoso en latinoamérica\".lower().split()\n",
    "\n",
    "tagger = \"/mnt/c/Users/ericj/Downloads/.../spanish.tagger\"\n",
    "jarfile = \"/mnt/c/Users/ericj/Downloads/.../stanford-postagger.jar\"\n",
    "\n",
    "tagger = StanfordPOSTagger(tagger, jarfile)\n",
    "tags = tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f444f1",
   "metadata": {},
   "source": [
    "# Word & Sentence Tokenizing in spanish\n",
    "**Sentence**: Utilizamos una\tinstancia\tde\t`PunktSentenceTokenizer` que\tfunciona\tpara\tuna\tgran\t\n",
    "variedad\tde\tidiomas.\n",
    "**Word**: usamos la librería `spaCy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a01071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk.data\n",
    "es_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle') # sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28965f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Quién eres tú? ¡Hola! ¿Dónde estoy?\n",
      "['¿Quién eres tú?', '¡Hola!', '¿Dónde estoy?']\n"
     ]
    }
   ],
   "source": [
    "txt = \"¿Quién eres tú? ¡Hola! ¿Dónde estoy?\"\n",
    "print(txt)\n",
    "print(es_tokenizer.tokenize(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9377b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "# En terminal: python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d554a719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('¿', 'PUNCT'), ('Quién', 'PRON'), ('eres', 'AUX'), ('tú', 'PRON'), ('?', 'PUNCT'), ('¡', 'PUNCT'), ('Hola', 'PROPN'), ('!', 'PUNCT'), ('¿', 'PUNCT'), ('Dónde', 'PRON'), ('estoy', 'AUX'), ('?', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "txt = \"¿Quién eres tú? ¡Hola! ¿Dónde estoy?\"\n",
    "doc = nlp(txt)\n",
    "print([(w.text, w.pos_) for w in doc])  # no da el POS más adecuado, quizás probar con md, lg o trf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a447b07",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "Using Snowball Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b2be932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'Las', 'propiedades', 'de', 'los', 'jesuitas', 'sirvieron', 'para', 'crear', 'nuevos', 'centros', '\\n', 'de', 'enseñanza', 'y', 'residencias', 'universitarias', ',', '¿', 'cierto', '?', '.', 'Sus', 'riquezas', ',', 'para', 'beneficiar', '\\n', 'a', 'los', 'sectores', 'más', 'necesitados', ',', 'se', 'destinaron', 'a', 'la', 'creación', 'de', 'hospitales', '\\n', 'y', 'hospicios', '.', 'Promovió', 'un', 'nuevo', 'plan', 'de', 'Estudios', 'Universitarios', ',', 'que', 'fue', '\\n', 'duramente', 'contestado', 'por', 'la', 'Universidad', 'de', 'Salamanca', ',', 'proponiendo', 'un', 'plan', '\\n', 'propio', ',', 'que', 'a', 'la', 'postre', 'fue', 'implantado', 'años', 'después', '.']\n"
     ]
    }
   ],
   "source": [
    "txt= \"\"\"\n",
    "Las propiedades de los jesuitas sirvieron para crear nuevos centros\n",
    "de enseñanza y residencias universitarias, ¿cierto?. Sus riquezas, para beneficiar \n",
    "a los sectores más necesitados, se destinaron a la creación de hospitales \n",
    "y hospicios. Promovió un nuevo plan de Estudios Universitarios, que fue \n",
    "duramente contestado por la Universidad de Salamanca, proponiendo un plan \n",
    "propio, que a la postre fue implantado años después.\"\"\"\n",
    "\n",
    "tokens = [w.text for w in nlp(txt)]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20af241d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prolong\n",
      "aprehend\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "print(stemmer.stem('prolongación'))\n",
    "print(stemmer.stem('aprehenderla'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e4369b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " 'las',\n",
       " 'propiedad',\n",
       " 'de',\n",
       " 'los',\n",
       " 'jesuit',\n",
       " 'sirv',\n",
       " 'par',\n",
       " 'cre',\n",
       " 'nuev',\n",
       " 'centr',\n",
       " '\\n',\n",
       " 'de',\n",
       " 'enseñ',\n",
       " 'y',\n",
       " 'resident',\n",
       " 'universitari',\n",
       " ',',\n",
       " '¿',\n",
       " 'ciert',\n",
       " '?',\n",
       " '.',\n",
       " 'sus',\n",
       " 'riquez',\n",
       " ',',\n",
       " 'par',\n",
       " 'benefici',\n",
       " '\\n',\n",
       " 'a',\n",
       " 'los',\n",
       " 'sector',\n",
       " 'mas',\n",
       " 'necesit',\n",
       " ',',\n",
       " 'se',\n",
       " 'destin',\n",
       " 'a',\n",
       " 'la',\n",
       " 'creacion',\n",
       " 'de',\n",
       " 'hospital',\n",
       " '\\n',\n",
       " 'y',\n",
       " 'hospici',\n",
       " '.',\n",
       " 'promov',\n",
       " 'un',\n",
       " 'nuev',\n",
       " 'plan',\n",
       " 'de',\n",
       " 'estudi',\n",
       " 'universitari',\n",
       " ',',\n",
       " 'que',\n",
       " 'fue',\n",
       " '\\n',\n",
       " 'dur',\n",
       " 'contest',\n",
       " 'por',\n",
       " 'la',\n",
       " 'univers',\n",
       " 'de',\n",
       " 'salamanc',\n",
       " ',',\n",
       " 'propon',\n",
       " 'un',\n",
       " 'plan',\n",
       " '\\n',\n",
       " 'propi',\n",
       " ',',\n",
       " 'que',\n",
       " 'a',\n",
       " 'la',\n",
       " 'postr',\n",
       " 'fue',\n",
       " 'implant',\n",
       " 'años',\n",
       " 'despues',\n",
       " '.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_text = [stemmer.stem(w) for w in tokens]\n",
    "stemmed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72425fb",
   "metadata": {},
   "source": [
    "# Lemmatizing\n",
    "Suele ser más útil que el stemming.\n",
    "Puede hacerse con SpaCy, FreeLing, CLARIN, Stanford CoreNLP\n",
    "\n",
    "En este caso, utilizamos Spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c823309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(txt)\n",
    "# cada uno de los objetos en el doc tiene diferentes propiedades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "73e4b191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> \n",
      "\n",
      "Las -> el\n",
      "propiedades -> propiedad\n",
      "de -> de\n",
      "los -> el\n",
      "jesuitas -> jesuita\n",
      "sirvieron -> servir\n",
      "para -> para\n",
      "crear -> crear\n",
      "nuevos -> nuevo\n",
      "centros -> centro\n",
      "\n",
      " -> \n",
      "\n",
      "de -> de\n",
      "enseñanza -> enseñanza\n",
      "y -> y\n",
      "residencias -> residencia\n",
      "universitarias -> universitario\n",
      ", -> ,\n",
      "¿ -> ¿\n",
      "cierto -> cierto\n",
      "? -> ?\n",
      ". -> .\n",
      "Sus -> su\n",
      "riquezas -> riqueza\n",
      ", -> ,\n",
      "para -> para\n",
      "beneficiar -> beneficiar\n",
      "\n",
      " -> \n",
      "\n",
      "a -> a\n",
      "los -> el\n",
      "sectores -> sector\n",
      "más -> más\n",
      "necesitados -> necesitado\n",
      ", -> ,\n",
      "se -> él\n",
      "destinaron -> destinar\n",
      "a -> a\n",
      "la -> el\n",
      "creación -> creación\n",
      "de -> de\n",
      "hospitales -> hospital\n",
      "\n",
      " -> \n",
      "\n",
      "y -> y\n",
      "hospicios -> hospicio\n",
      ". -> .\n",
      "Promovió -> promover\n",
      "un -> uno\n",
      "nuevo -> nuevo\n",
      "plan -> plan\n",
      "de -> de\n",
      "Estudios -> Estudios\n",
      "Universitarios -> Universitarios\n",
      ", -> ,\n",
      "que -> que\n",
      "fue -> ser\n",
      "\n",
      " -> \n",
      "\n",
      "duramente -> duramente\n",
      "contestado -> contestado\n",
      "por -> por\n",
      "la -> el\n",
      "Universidad -> Universidad\n",
      "de -> de\n",
      "Salamanca -> Salamanca\n",
      ", -> ,\n",
      "proponiendo -> proponer\n",
      "un -> uno\n",
      "plan -> plan\n",
      "\n",
      " -> \n",
      "\n",
      "propio -> propio\n",
      ", -> ,\n",
      "que -> que\n",
      "a -> a\n",
      "la -> el\n",
      "postre -> postre\n",
      "fue -> ser\n",
      "implantado -> implantar\n",
      "años -> año\n",
      "después -> después\n",
      ". -> .\n"
     ]
    }
   ],
   "source": [
    "for w in doc:\n",
    "    print(w, '->', w.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a9f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "738100a9",
   "metadata": {},
   "source": [
    "# Text Clustering\n",
    "\n",
    "- [Multilingual LDA Pipeline - ArticiAI](https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA)\n",
    "- [Tutorial](https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA/blob/master/Multilingual-LDA-Pipeline-Tutorial.ipynb)\n",
    "\n",
    "Before using any lackage you must do some text pre-procesing on the corpus\n",
    "\n",
    "- tokenization\n",
    "- normalization (e.g. lowercase)\n",
    "- stop-word removal\n",
    "- stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588df268",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
