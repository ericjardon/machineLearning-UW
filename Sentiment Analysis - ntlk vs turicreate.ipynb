{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis:\n",
    "\n",
    "En este Jupyter Notebook nos empeñamos en realizar una comparación entre dos métodos de análisis de sentimiento usando regresión logística.\n",
    "\n",
    "Las implementaciones a evaluar son:\n",
    "- `LogisticRegression` de `sklearn`\n",
    "- `logistic_classifier.create` de `turicreate`\n",
    "\n",
    "Para comparabilidad se utiliza el mismo conjunto de datos: \"amazon_baby\" que contiene **183531** reseñas de productos para bebé en Amazon en inglés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Using sklearn's `LogisticRegression`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_306/913694509.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Read the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/amazon_baby.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# PASO 1) CARGAR LOS DATOS\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv('./data/amazon_baby.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Planetwise Wipe Pouch</td>\n",
       "      <td>it came early and was not disappointed. i love...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annas Dream Full Quilt with 2 Shams</td>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>This is a product well worth the purchase.  I ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>When the Binky Fairy came to our house, we did...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A Tale of Baby's Days with Peter Rabbit</td>\n",
       "      <td>Lovely book, it's bound tightly so you may not...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Baby Tracker&amp;reg; - Daily Childcare Journal, S...</td>\n",
       "      <td>Perfect for new parents. We were able to keep ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Baby Tracker&amp;reg; - Daily Childcare Journal, S...</td>\n",
       "      <td>A friend of mine pinned this product on Pinter...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Baby Tracker&amp;reg; - Daily Childcare Journal, S...</td>\n",
       "      <td>This has been an easy way for my nanny to reco...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Baby Tracker&amp;reg; - Daily Childcare Journal, S...</td>\n",
       "      <td>I love this journal and our nanny uses it ever...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name  \\\n",
       "1                               Planetwise Wipe Pouch   \n",
       "2                 Annas Dream Full Quilt with 2 Shams   \n",
       "3   Stop Pacifier Sucking without tears with Thumb...   \n",
       "4   Stop Pacifier Sucking without tears with Thumb...   \n",
       "5   Stop Pacifier Sucking without tears with Thumb...   \n",
       "6             A Tale of Baby's Days with Peter Rabbit   \n",
       "7   Baby Tracker&reg; - Daily Childcare Journal, S...   \n",
       "8   Baby Tracker&reg; - Daily Childcare Journal, S...   \n",
       "9   Baby Tracker&reg; - Daily Childcare Journal, S...   \n",
       "10  Baby Tracker&reg; - Daily Childcare Journal, S...   \n",
       "\n",
       "                                               review  rating  sentiment  \n",
       "1   it came early and was not disappointed. i love...       5          1  \n",
       "2   Very soft and comfortable and warmer than it l...       5          1  \n",
       "3   This is a product well worth the purchase.  I ...       5          1  \n",
       "4   All of my kids have cried non-stop when I trie...       5          1  \n",
       "5   When the Binky Fairy came to our house, we did...       5          1  \n",
       "6   Lovely book, it's bound tightly so you may not...       4          1  \n",
       "7   Perfect for new parents. We were able to keep ...       5          1  \n",
       "8   A friend of mine pinned this product on Pinter...       5          1  \n",
       "9   This has been an easy way for my nanny to reco...       4          1  \n",
       "10  I love this journal and our nanny uses it ever...       4          1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove any 'neutral' ratings equal to 3\n",
    "df = df[df['rating'] != 3]\n",
    "\n",
    "# Encode 4s and 5s as 1 (positive reviews)\n",
    "# Encode 1s and 2s as 0 (negative reviews)\n",
    "df['sentiment'] = np.where(df['rating'] > 3, 1, 0)\n",
    "df.head(10)\n",
    "\n",
    "## NOTE --- should we remove punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166752"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and test sets\n",
    "# what percentage is used?\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(df['review'], \n",
    "                                                    df['sentiment'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train first entry:\n",
      "\n",
      " So far so good.  My baby is yet to come and try it out herself, but for the time spent on building the crib and the look/feel of it, I give it 5 stars.  Mattress fits well.  Good safety tips provided.  Instructions were at times a little hard to follow but if you look at the pictures you can't go wrong.  The crib is sturdy, yet light enough to be moved around (we've already done that twice!).  Bought the espresso finish and it goes well with the nursery theme.  It does include the toddler rail.  I was also impressed with the packing; they seem to care about the product.  I preferred this model over the Emily although they are similar and the Emily is cheaper - I didn't like the gap in the rail for when my baby grows older and would be able to stand up in the crib and put her hand through it.  Of course you can get suitable covers for the gap, but less the material surrounding your baby, the better!\n",
      "\n",
      "\n",
      "X_train shape:  (125064,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train first entry:\\n\\n', Xtrain.iloc[0])\n",
    "print('\\n\\nX_train shape: ', Xtrain.shape)\n",
    "\n",
    "# 125064 observations in sklearn training set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word count vectors with CountVectorizer\n",
    "CountVectorizer is a tool provided by scikit-learn to convert a given text into a vector (array) of word frequency counts.\n",
    "\n",
    "Count Vectorizer is helpful when we have multiple texts to analyze, and we wish to convert each word in each text into vectors put into a matrix.\n",
    "\n",
    "CountVectorizer creates a matrix in which each unique word is represented by a column of the matrix, and each text sample from the document is a row in the matrix. The value of each cell is nothing but the count of the word in that particular text sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Obtain the matrix of word counts (observations as rows, words as columns)\n",
    "vect = CountVectorizer().fit(Xtrain.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55486"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.get_feature_names()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<125064x55486 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6620576 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform the documents in the training data to a document-term matrix\n",
    "Xtrain_vectorized = vect.transform(Xtrain.values.astype('U'))\n",
    "Xtrain_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the word count model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/echao/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the model with training set and training labels\n",
    "model = LogisticRegression()\n",
    "model.fit(Xtrain_vectorized, ytrain)\n",
    "\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model.predict(vect.transform(Xtest.values.astype('U')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining Performance metrics for Word count model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8472402772762485\n",
      "Precision:  0.9490078643194985\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_score\n",
    "\n",
    "\n",
    "print('AUC: ', roc_auc_score(ytest, predictions))\n",
    "\n",
    "print('Precision: ', precision_score(ytest, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word count Extreme words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['disappointing' 'worst' 'concept' 'poorly' 'worthless' 'useless'\n",
      " 'horrible' 'returning' 'disappointment' 'shame']\n",
      "\n",
      "Largest Coefs: \n",
      "['excellent' 'worry' 'awesome' 'amazing' 'satisfied' 'negative'\n",
      " 'fantastic' 'complaints' 'pleased' 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/echao/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names_out())\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1] \n",
    "# so the list returned is in order of largest to smallest\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16962"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Minimum document frequency 5\n",
    "vect = TfidfVectorizer(min_df=5).fit(Xtrain.values.astype('U'))\n",
    "len(vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/echao/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "Xtrain_vectorized = vect.transform(Xtrain.values.astype('U'))\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(Xtrain_vectorized, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of Tf-idf vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8323408454703418\n",
      "Precision:  0.9425091418987654\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(vect.transform(Xtest.values.astype('U')))\n",
    "\n",
    "\n",
    "print('AUC: ', roc_auc_score(ytest, predictions))\n",
    "print('Precision: ', precision_score(ytest, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extreme words Tf-Idf vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest tfidf:\n",
      "['emotionally' 'routed' 'paragraph' 'consi' 'court' 'ly' '249' 'thumping'\n",
      " 'bumpier' 'pds']\n",
      "\n",
      "Largest tfidf: \n",
      "['awesome' 'nice' 'flimsy' 'love' 'good' 'works' 'gracias' 'excellent'\n",
      " 'excelente' 'excelent']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names_out())\n",
    "\n",
    "sorted_tfidf_index = Xtrain_vectorized.max(0).toarray()[0].argsort()\n",
    "\n",
    "print('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['not' 'disappointed' 'returned' 'useless' 'returning' 'return' 'waste'\n",
      " 'disappointing' 'poor' 'worst']\n",
      "\n",
      "Largest Coefs: \n",
      "['love' 'great' 'easy' 'perfect' 'loves' 'best' 'perfectly' 'highly'\n",
      " 'happy' 'glad']\n"
     ]
    }
   ],
   "source": [
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "# These reviews are treated the same by our current model\n",
    "print(model.predict(vect.transform(['not an issue, phone is working',\n",
    "                                    'an issue, phone is not working'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190992"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the CountVectorizer to the training data specifiying a minimum \n",
    "# document frequency of 5 and extracting 1-grams and 2-grams\n",
    "vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(Xtrain.values.astype('U'))\n",
    "\n",
    "Xtrain_vectorized = vect.transform(Xtrain.values.astype('U'))\n",
    "\n",
    "len(vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/echao/projects/machineLearning-UW/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(Xtrain_vectorized, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance for N-Gram Count Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8919209034316948\n",
      "Precision:  0.9639553249097473\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(vect.transform(Xtest.values.astype('U')))\n",
    "\n",
    "print('AUC: ', roc_auc_score(ytest, predictions))\n",
    "print('Precision: ', precision_score(ytest, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extreme words n-gram count vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['not worth' 'not recommend' 'disappointing' 'two stars' 'not happy'\n",
      " 'to love' 'wouldn recommend' 'useless' 'very disappointed' 'worst']\n",
      "\n",
      "Largest Coefs: \n",
      "['my only' 'not too' 'excellent' 'perfect' 'awesome' 'not leak'\n",
      " 'be disappointed' 'just what' 'high quality' 'fantastic']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names_out())\n",
    "\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "# These reviews are now correctly identified\n",
    "print(model.predict(vect.transform(['not an issue, phone is working',\n",
    "                                    'an issue, phone is not working'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166752"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xtest) + len(Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Using Turi Create's `logistic_regression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import turicreate\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">name</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">review</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Planetwise Flannel Wipes</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">These flannel wipes are<br>OK, but in my opinion ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Planetwise Wipe Pouch</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">it came early and was not<br>disappointed. i love ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Annas Dream Full Quilt<br>with 2 Shams ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Very soft and comfortable<br>and warmer than it ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Stop Pacifier Sucking<br>without tears with ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">This is a product well<br>worth the purchase.  I ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Stop Pacifier Sucking<br>without tears with ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">All of my kids have cried<br>non-stop when I tried to ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Stop Pacifier Sucking<br>without tears with ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">When the Binky Fairy came<br>to our house, we didn&#x27;t ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">A Tale of Baby&#x27;s Days<br>with Peter Rabbit ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Lovely book, it&#x27;s bound<br>tightly so you may no ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Baby Tracker&amp;reg; - Daily<br>Childcare Journal, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Perfect for new parents.<br>We were able to keep ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Baby Tracker&amp;reg; - Daily<br>Childcare Journal, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">A friend of mine pinned<br>this product on Pinte ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Baby Tracker&amp;reg; - Daily<br>Childcare Journal, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">This has been an easy way<br>for my nanny to record ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4.0</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[183531 rows x 3 columns]<br/>Note: Only the head of the SFrame is printed.<br/>You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tname\tstr\n",
       "\treview\tstr\n",
       "\trating\tfloat\n",
       "\n",
       "Rows: 183531\n",
       "\n",
       "Data:\n",
       "+-------------------------------+-------------------------------+--------+\n",
       "|              name             |             review            | rating |\n",
       "+-------------------------------+-------------------------------+--------+\n",
       "|    Planetwise Flannel Wipes   | These flannel wipes are OK... |  3.0   |\n",
       "|     Planetwise Wipe Pouch     | it came early and was not ... |  5.0   |\n",
       "| Annas Dream Full Quilt wit... | Very soft and comfortable ... |  5.0   |\n",
       "| Stop Pacifier Sucking with... | This is a product well wor... |  5.0   |\n",
       "| Stop Pacifier Sucking with... | All of my kids have cried ... |  5.0   |\n",
       "| Stop Pacifier Sucking with... | When the Binky Fairy came ... |  5.0   |\n",
       "| A Tale of Baby's Days with... | Lovely book, it's bound ti... |  4.0   |\n",
       "| Baby Tracker&reg; - Daily ... | Perfect for new parents. W... |  5.0   |\n",
       "| Baby Tracker&reg; - Daily ... | A friend of mine pinned th... |  5.0   |\n",
       "| Baby Tracker&reg; - Daily ... | This has been an easy way ... |  4.0   |\n",
       "+-------------------------------+-------------------------------+--------+\n",
       "[183531 rows x 3 columns]\n",
       "Note: Only the head of the SFrame is printed.\n",
       "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = turicreate.SFrame('./data/amazon_baby.sframe/')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166752"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove neutral reviews\n",
    "data = data[data['rating'] != 3]\n",
    "\n",
    "len(data) # 166,752"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = text.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Create array of punctuation-less reviews\n",
    "review_without_punctuation = data['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word count column from every punctuation-less review\n",
    "data['word_count'] = turicreate.text_analytics.count_words(review_without_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment'] = data['rating'].apply(lambda r: +1 if r>3 else -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearns test-train split does 75-25\n",
    "train_data, test_data = data.random_split(0.75, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41630\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Logistic Classifier with Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Logistic regression:</pre>"
      ],
      "text/plain": [
       "Logistic regression:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of examples          : 125122</pre>"
      ],
      "text/plain": [
       "Number of examples          : 125122"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of classes           : 2</pre>"
      ],
      "text/plain": [
       "Number of classes           : 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of feature columns   : 1</pre>"
      ],
      "text/plain": [
       "Number of feature columns   : 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of unpacked features : 116753</pre>"
      ],
      "text/plain": [
       "Number of unpacked features : 116753"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of coefficients      : 116754</pre>"
      ],
      "text/plain": [
       "Number of coefficients      : 116754"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Starting L-BFGS</pre>"
      ],
      "text/plain": [
       "Starting L-BFGS"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+-----------+--------------+-------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+-----------+--------------+-------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| Iteration | Passes   | Step size | Elapsed Time | Training Accuracy |</pre>"
      ],
      "text/plain": [
       "| Iteration | Passes   | Step size | Elapsed Time | Training Accuracy |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+-----------+--------------+-------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+-----------+--------------+-------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 0         | 4        | 0.250000  | 2.078150     | 0.841571          |</pre>"
      ],
      "text/plain": [
       "| 0         | 4        | 0.250000  | 2.078150     | 0.841571          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 1         | 9        | 3.250000  | 3.430734     | 0.940762          |</pre>"
      ],
      "text/plain": [
       "| 1         | 9        | 3.250000  | 3.430734     | 0.940762          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 2         | 11       | 2.799126  | 4.045188     | 0.943535          |</pre>"
      ],
      "text/plain": [
       "| 2         | 11       | 2.799126  | 4.045188     | 0.943535          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 3         | 12       | 2.799126  | 4.462575     | 0.968463          |</pre>"
      ],
      "text/plain": [
       "| 3         | 12       | 2.799126  | 4.462575     | 0.968463          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 4         | 13       | 2.799126  | 4.852427     | 0.977902          |</pre>"
      ],
      "text/plain": [
       "| 4         | 13       | 2.799126  | 4.852427     | 0.977902          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 5         | 14       | 2.799126  | 5.266645     | 0.977902          |</pre>"
      ],
      "text/plain": [
       "| 5         | 14       | 2.799126  | 5.266645     | 0.977902          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+-----------+--------------+-------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+-----------+--------------+-------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Turi Create logistic classifier implementation.\n",
    "\n",
    "sentiment_model = turicreate.logistic_classifier.create(train_data,\n",
    "                                                        target = 'sentiment',\n",
    "                                                        features=['word_count'],\n",
    "                                                        validation_set=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116754"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiment_model.coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-------+--------------------+--------+\n",
      "|    name    |       index       | class |       value        | stderr |\n",
      "+------------+-------------------+-------+--------------------+--------+\n",
      "| word_count |       offsi       |   1   | 22.24657743326877  |  None  |\n",
      "| word_count |     apathetic     |   1   | 21.940532564887228 |  None  |\n",
      "| word_count | intentionalbuying |   1   | 21.940532564887228 |  None  |\n",
      "| word_count |   pullingguiding  |   1   | 21.940532564887228 |  None  |\n",
      "| word_count |     inversely     |   1   | 21.940532564887228 |  None  |\n",
      "| word_count |    unsurvivable   |   1   | 21.940532564887228 |  None  |\n",
      "| word_count |    strongermore   |   1   | 21.940532564887228 |  None  |\n",
      "| word_count |     betterive     |   1   | 21.940532564887228 |  None  |\n",
      "| word_count |    featurebelt    |   1   | 21.940532564887228 |  None  |\n",
      "| word_count |     restupper     |   1   | 21.940532564887228 |  None  |\n",
      "+------------+-------------------+-------+--------------------+--------+\n",
      "[116754 rows x 5 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights.sort('value', ascending=False).print_rows(num_rows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+-------+--------------------+--------+\n",
      "|    name    |     index      | class |       value        | stderr |\n",
      "+------------+----------------+-------+--------------------+--------+\n",
      "| word_count |  chairsthere   |   1   | -32.12151548266517 |  None  |\n",
      "| word_count |     winsof     |   1   |  -22.731830657705  |  None  |\n",
      "| word_count |    leachops    |   1   |  -22.731830657705  |  None  |\n",
      "| word_count |   aboutmommy   |   1   | -22.34199389422742 |  None  |\n",
      "| word_count |   superibibs   |   1   | -22.34199389422742 |  None  |\n",
      "| word_count | countertoptray |   1   | -22.34199389422742 |  None  |\n",
      "| word_count | carrierupdate  |   1   | -22.34199389422742 |  None  |\n",
      "| word_count |   rubberthe    |   1   | -22.34199389422742 |  None  |\n",
      "| word_count |    towelin     |   1   | -22.34199389422742 |  None  |\n",
      "| word_count |  resultsboth   |   1   | -22.34199389422742 |  None  |\n",
      "+------------+----------------+-------+--------------------+--------+\n",
      "[116754 rows x 5 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights.sort('value', ascending=True).print_rows(num_rows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = sentiment_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9203699255344703"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_classification_accuracy(model, data, true_labels):\n",
    "    N = len(true_labels)\n",
    "    # First get the predictions\n",
    "    predictions = model.predict(data)\n",
    "    \n",
    "    # Compute the number of correctly classified examples\n",
    "    num_correct = 0\n",
    "    for i in range(N):\n",
    "        num_correct += (predictions[i] == true_labels[i])\n",
    "    # Then compute accuracy by dividing num_correct by total number of examples\n",
    "    accuracy = num_correct / N\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "get_classification_accuracy(sentiment_model, test_data, test_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6573432644224521"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = sentiment_model.predict(test_data)\n",
    "predictions_norm = predictions.apply(lambda x: 0 if x<0 else 1)\n",
    "\n",
    "turicreate.evaluation.auc(test_data['sentiment'], predictions_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conlcusiones\n",
    "\n",
    "\n",
    "Los modelos de nltk tienen mejor desempeño (y además tokenizan de forma automática el texto), a condición de que se cuente con un conjunto de datos suficiente.\n",
    "\n",
    "El conjunto de palabras más significativas en ambos extremos por parte de turi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
